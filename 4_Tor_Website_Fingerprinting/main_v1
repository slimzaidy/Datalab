from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
import gzip
import shutil
from sklearn.pipeline import Pipeline
from scapy.utils import RawPcapReader
from zipfile import ZipFile
from sklearn.preprocessing import OneHotEncoder

def process_pcap(file_name):
    print('Opening {}...'.format(file_name))
    count = 0
    for (pkt_data) in RawPcapReader(file_name):
        count += 1
    print('{} contains {} packets'.format(file_name, count))
    packet_array.append(count)




""" initialization the variables"""
labels = []
features = []
size_array = []
packet_array = []


file_name = r"C:\Users\oussa\Downloads\datalab\\train.zip"
with ZipFile(file_name, 'r') as zip:
    #zip.printdir()
    print("extractin all files!")
    #zip.extractall()
    print("done")
    for name in zip.namelist():
        print(name)
        process_pcap(name)
        label_begin_index = name.rfind('_')
        label_end_index = name.rfind('.')
        feature_begin_index = name.rfind('/')
        label = int(name[label_begin_index + 1: label_end_index])
        labels.append(label)
        features.append(name[feature_begin_index+1:label_begin_index])





""" Create packet size array"""
packet_array = [round(element/512) for element in packet_array]

""" Calculate the number of packets"""





"""
rbf = svm.SVC(kernel='rbf', gamma = 0.4, C = 0.1).fit(X_train,y_train)
poly = svm.SVC(kernel='poly', degree = 3, C = 0.1).fit(X_train,y_train)

poly_pred = poly.predict(X_test)
rbf_pred = rbf.predict(X_test)
print("Poly SVM Accuracy", metrics.accuracy_score(y_test, poly_pred))
print("rbf SVM Accuracy", metrics.accuracy_score(y_test, rbf_pred))
"""



"""split the train test data"""
enc = OneHotEncoder()
feat1 = enc.fit(features)
OneHotEncoder(categorical_features='all', handle_unknown='error', n_values='auto', sparse=True)
X_train, X_test, y_train, y_test = train_test_split(feat1,labels, test_size=0.3, shuffle = True)


"""implement the SVM model"""
params_grid = [{'kernel': ['rbf','poly'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4],
                     'C': [1, 10, 50, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]
svm_model = GridSearchCV(SVC(), params_grid, cv=5)
svm_model.fit(X_train, y_train)
final_svm_model = svm_model.best_estimator_
svm_pred = final_svm_model.predict(X_test)
print("Poly SVM Accuracy", metrics.accuracy_score(y_test, svm_pred))




"""implement the KNN model"""
#model = KNeighborsClassifier(n_neighbors= 5)
#model.fit(features, labels)
#y_pred = model.predict(X_test)
#print("Accuracy", metrics.accuracy_score(y_test, y_pred))

""" Implement the Decision tree model"""
#clf = DecisionTreeClassifier()
#clf = clf.fit(X_train, y_train)
#y_pred = clf.predict(X_test)



"""
# <Mohammad Saidur Rahman><mr6564@rit.edu>
import os

def extract(times, sizes, features):

    #Transmission size features
    features.append(len(times))

    count = 0
    for x in sizes:
        if x > 0:
            count += 1
    features.append(count)
    features.append(len(times)-count)

    features.append(times[-1] - times[0])


    #Transpositions (similar to good distance scheme)
    count = 0
    for i in range(0, len(sizes)):
        if sizes[i] > 0:
            count += 1
            features.append(i)
        if count == 300:
            break
    for i in range(count, 300):
        features.append(0)

    count = 0
    prevloc = 0
    for i in range(0, len(sizes)):
        if sizes[i] > 0:
            count += 1
            features.append(i - prevloc)
            prevloc = i
        if count == 300:
            break
    for i in range(count, 300):
        features.append(0)


    #Packet distributions (where are the outgoing packets concentrated)
    for i in range(0, min(len(sizes), 3000)):
        if i % 30 != 29:
            if sizes[i] > 0:
                count += 1
        else:
            features.append(count)
            count = 0
    for i in range(len(sizes)/30, 100):
        features.append(0)

    #Bursts
    bursts = []
    curburst = 0
    stopped = 0
    for x in sizes:
        if x < 0:
            stopped = 0
            curburst -= x
        if x > 0 and stopped == 0:
            stopped = 1
        if x > 0 and stopped == 1:
            stopped = 0
            bursts.append(curburst)
    features.append(max(bursts))
    features.append(sum(bursts)/len(bursts))
    features.append(len(bursts))
    counts = [0, 0, 0]
    for x in bursts:
        if x > 5:
            counts[0] += 1
        if x > 10:
            counts[1] += 1
        if x > 15:
            counts[2] += 1
    features.append(counts[0])
    features.append(counts[1])
    features.append(counts[2])
    for i in range(0, 5):
        try:
            features.append(bursts[i])
        except:
            features.append(0)

    for i in range(0, 20):
        try:
            features.append(sizes[i] + 1500)
        except:
            features.append(0)

#this will create a new file in a new directory KNN_SVM named knnsvmtest.
#We have to feed this new file location to SVM classifier.
fdout = open(os.path.join("/home/mr6564/Desktop/WebsiteFingerprinting/KNN_SVM/",'knnsvmtest'), 'w')


#this takes quite a while
for site in range(0, 100):
    #print site
    for instance in range(0, 90):
        fname = str(site) + "-" + str(instance)
        #Set up times, sizes
        f = open("/home/mr6564/Desktop/WebsiteFingerprinting/KNN_SVM/batch/" + fname, "r")
        times = []
        sizes = []
        for x in f:
            x = x.split("\t")
            times.append(float(x[0]))
            sizes.append(int(x[1]))
        f.close()
        #Extract features. All features are non-negative numbers or X.
        features = []
        extract(times, sizes, features)



        fdout.write(str(site) + ' ' + ' '.join(['%d:%s' % (i + 1, el) for i, el in enumerate(features)]) + '\n')
fdout.close()
"""
