#######################################################################
# Datalab
# Schadcode f√ºr Webserver
# Gruppe: Hex Haxors
# Zaid Askari & Oussema Ben Taarit
#######################################################################
# Improting the initial necessary libraries

import numpy as np
import zipfile
import pandas as pd
from pandas.core.frame import DataFrame
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import balanced_accuracy_score
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

import seedir as sd
from io import BytesIO
import os
import re
import urlextract 
import nltk
#######################################################################
# Define the variables 
labels = []

#######################################################################
# Extract the text from each document

train_zip = zipfile.ZipFile("3_Schadcode_in_software/train.zip")
names = train_zip.namelist() 
# removing the labels file
names = names[:-1]
contents = []
#print(names)
url_extractor = urlextract.URLExtract()
stemmer = nltk.PorterStemmer()


for name in names:
    #print(name)
    tokens = name.split(".")
    labels.append(int(tokens[1]))

for i in range(0, 5): #len(names))
    #print(type(name))
    content = train_zip.read(names[i]).decode("latin-1")
    #print(test.decode("ISO-8859-1"))
    urls = list(set(url_extractor.find_urls(content)))
    urls.sort(key=lambda url: len(url), reverse=True)
    for url in urls:
        content = content.replace(url, " URL ")
        #print(url)
    content = re.sub(r'\d+(?:\.\d*)?(?:[eE][+-]?\d+)?', ' NUMBER ', content)
    # words = re.split("\\s+", content)
    # stemmed_words = [stemmer.stem(word=word) for word in words]
    # print(stemmed_words)
    # content = ' '.join(stemmed_words)
    contents.append(content)

print(len(contents[0]))
print(len(contents[1]))
#print(type(contents[0]))
# print(type(names[0]))
#content = [train_zip.read(names[0]).decode("latin-1")]
#print(content)
vectorizer = CountVectorizer(lowercase = True, max_df = 0.25, analyzer = 'word', ngram_range = (1, 1)) #max_df = 0.5 #max_features=1000
content_updated = vectorizer.fit_transform(contents)
#print(vectorizer.vocabulary_)
#print(type(content_updated[0]))

print(content_updated[0].shape)
print(content_updated[1].shape)

train_zip.close()


####### Clustering ###########

df = pd.DataFrame(content_updated.toarray(), columns=vectorizer.get_feature_names())
#print(df)

n_clusters = 4
kmeans = KMeans(n_clusters) 
y_pred = kmeans.fit_predict(df)
u_labels = np.unique(y_pred)

print("Top terms per cluster:")
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names()
for i in range(n_clusters):
    top_ten_words = [terms[ind] for ind in order_centroids[i, :5]]
    print("Cluster {}: {}".format(i, ' '.join(top_ten_words)))
print(kmeans.labels_)
print(kmeans.cluster_centers_)

pca = PCA(n_components=2)
table = pd.DataFrame()
table['x'] = pca.fit_transform(kmeans.cluster_centers_)[0]
table['y'] = pca.fit_transform(kmeans.cluster_centers_)[1]
print(table)


colormap = {
    0: 'red',
    1: 'green',
    2: 'blue',
    3: 'black'
}
colors = table.apply(lambda row: colormap[row.category], axis=1)

# ax = df.plot(kind='scatter', x=table.x, y=table.y, alpha=0.2, s=200)
# ax.set_xlabel("Fish")
# ax.set_ylabel("Penny")
# plt.show()
# plt.close()